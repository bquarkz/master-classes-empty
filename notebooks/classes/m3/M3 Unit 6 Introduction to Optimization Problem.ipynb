{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "955e2637",
   "metadata": {},
   "source": [
    "# Gradient Descent Method (First Principle) Demonstration In Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d916f51e",
   "metadata": {},
   "source": [
    "## Background \n",
    "Linear regression is a fundamental machine learning algorithm used to model the relationship between a dependent variable (y) and independent variable(s) (x). Instead of using built-in libraries, we will implement linear regression from first principles using gradient descent optimization. This approach helps understand how machine learning algorithms work under the hood by:\n",
    "\n",
    "- Defining a loss function (sum of squared errors)\n",
    "- Computing gradients (partial derivatives)\n",
    "- Iteratively updating parameters to minimize loss\n",
    "\n",
    "Our objective is to find optimal parameters (b0, b1) for the equation y = b0 + b1x that best fit our data, and validate our implementation against statsmodels OLS to ensure correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb35e7b5",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87779c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f455aa9c",
   "metadata": {},
   "source": [
    "#### Note : For this demonstration, we'll use a simple synthetic dataset with 10 data points (X from 1 to 10). This allows us to focus on understanding how gradient descent works without worrying about data complexities. The same principles apply to real-world datasets - you would just need to adjust the learning rate and possibly use feature scaling for faster convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9de04d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000: b0 = 0.2155, b1 = 1.9785, Loss = 0.3159\n",
      "Iteration 2000: b0 = 0.1713, b1 = 1.9848, Loss = 0.2956\n",
      "Iteration 3000: b0 = 0.1423, b1 = 1.9890, Loss = 0.2869\n",
      "Iteration 4000: b0 = 0.1232, b1 = 1.9917, Loss = 0.2831\n",
      "Iteration 5000: b0 = 0.1107, b1 = 1.9935, Loss = 0.2815\n",
      "Iteration 6000: b0 = 0.1024, b1 = 1.9947, Loss = 0.2808\n",
      "Iteration 7000: b0 = 0.0970, b1 = 1.9955, Loss = 0.2805\n",
      "Iteration 8000: b0 = 0.0935, b1 = 1.9960, Loss = 0.2803\n",
      "Iteration 9000: b0 = 0.0911, b1 = 1.9963, Loss = 0.2803\n",
      "Iteration 10000: b0 = 0.0896, b1 = 1.9965, Loss = 0.2803\n",
      "Iteration 11000: b0 = 0.0886, b1 = 1.9967, Loss = 0.2803\n",
      "Iteration 12000: b0 = 0.0879, b1 = 1.9968, Loss = 0.2802\n",
      "Iteration 13000: b0 = 0.0875, b1 = 1.9968, Loss = 0.2802\n",
      "Iteration 14000: b0 = 0.0872, b1 = 1.9969, Loss = 0.2802\n",
      "Iteration 15000: b0 = 0.0870, b1 = 1.9969, Loss = 0.2802\n",
      "Iteration 16000: b0 = 0.0869, b1 = 1.9969, Loss = 0.2802\n",
      "Iteration 17000: b0 = 0.0868, b1 = 1.9969, Loss = 0.2802\n",
      "Iteration 18000: b0 = 0.0868, b1 = 1.9970, Loss = 0.2802\n",
      "Iteration 19000: b0 = 0.0867, b1 = 1.9970, Loss = 0.2802\n",
      "Iteration 20000: b0 = 0.0867, b1 = 1.9970, Loss = 0.2802\n",
      "Iteration 21000: b0 = 0.0867, b1 = 1.9970, Loss = 0.2802\n",
      "\n",
      "Converged at iteration 21492!\n",
      "Loss improvement: 0.0000000000 < tolerance: 1e-12\n",
      "\n",
      "Final Parameters:\n",
      "b0 (intercept) = 0.0867\n",
      "b1 (slope) = 1.9970\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "y = np.array([2.1, 4.2, 5.8, 8.1, 10.3, 11.9, 14.2, 16.1, 17.8, 20.2])\n",
    "\n",
    "\n",
    "# Initialize parameters\n",
    "b0 = 0.0  # intercept\n",
    "b1 = 0.0  # slope\n",
    "learning_rate = 0.001  # Increased learning rate for faster convergence\n",
    "iterations = 100000 # Increased iterations\n",
    "n = len(X)  # number of data points\n",
    "\n",
    "# Lists to store loss history\n",
    "loss_history = []\n",
    "\n",
    "# Gradient Descent with early stopping\n",
    "tolerance = 1e-12  # Reduced tolerance for better convergence\n",
    "previous_loss = float('inf')\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Calculate predictions and errors\n",
    "    y_pred = b0 + b1 * X\n",
    "    ei = y - y_pred\n",
    "    \n",
    "    # Calculate loss (for tracking)\n",
    "    loss = np.sum(ei ** 2)\n",
    "    loss_history.append(loss)\n",
    "    \n",
    "    # Check if loss is minimized (converged)\n",
    "    if abs(previous_loss - loss) < tolerance:\n",
    "        print(f\"\\nConverged at iteration {i+1}!\")\n",
    "        print(f\"Loss improvement: {abs(previous_loss - loss):.10f} < tolerance: {tolerance}\")\n",
    "        break\n",
    "    \n",
    "    previous_loss = loss\n",
    "    \n",
    "    # Calculate gradients (corrected with averaging)\n",
    "    dL_db0 = (-2 / n) * np.sum(ei)\n",
    "    dL_db1 = (-2 / n) * np.sum(ei * X)\n",
    "    \n",
    "    # Update parameters\n",
    "    b0 = b0 - learning_rate * dL_db0\n",
    "    b1 = b1 - learning_rate * dL_db1\n",
    "    \n",
    "    # Print progress every 100 iterations\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print(f\"Iteration {i+1}: b0 = {b0:.4f}, b1 = {b1:.4f}, Loss = {loss:.4f}\")\n",
    "\n",
    "# Final results\n",
    "print(f\"\\nFinal Parameters:\")\n",
    "print(f\"b0 (intercept) = {b0:.4f}\")\n",
    "print(f\"b1 (slope) = {b1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48ea646",
   "metadata": {},
   "source": [
    "### Optimum Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5210a167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Parameters:\n",
      "b0 (intercept) = 0.0867\n",
      "b1 (slope) = 1.9970\n"
     ]
    }
   ],
   "source": [
    "# Final results\n",
    "print(f\"\\nFinal Parameters:\")\n",
    "print(f\"b0 (intercept) = {b0:.4f}\")\n",
    "print(f\"b1 (slope) = {b1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
